library(e1071)
library(caTools) # for runmean

# before running extract, make sure train is in its gamebygame state
# or else you'll get duplicate columns
# see Explore_vX.R
extract <- function(data) {
	tmp <- as.data.frame(t(sapply(1:nrow(data), function(row) {
		val <- as.integer(unlist(strsplit(as.character(data$Valuations[row]), split=" ")))	
		GameLength <- length(which(!is.na(val)))
		# if (data$GameLength[row] <= 8) {
		if (GameLength <= 8) {
			return(c(modeNum=NA,kur=NA,skw=NA,avg=NA,med=NA,sdv=NA,rng=NA,iqr=NA))
		} else {
			val.wo6 <- val[-(1:6)]
			valDensity <- density(val.wo6, na.rm=TRUE)$y
			modeNum <- length(which(diff(sign(diff(valDensity)))==-2)+1) # http://stackoverflow.com/questions/6836409/finding-local-maxima-and-minima
			val.large <- ifelse(abs(val.wo6)>300, (val.wo6/abs(val.wo6))*(300+sqrt(abs(abs(val.wo6)-300))), val.wo6)
			val.parsed <- runmean(x=val.large, k=4, alg="C", endrule="trim")
			kur <- kurtosis(val.parsed, na.rm=TRUE)
			skw <- skewness(val.parsed, na.rm=TRUE)
			avg <- mean(val.parsed, na.rm=TRUE)
			med <- median(val.parsed, na.rm=TRUE)
			sdv <- sd(val.parsed, na.rm=TRUE)
			rng <- max(val.parsed, na.rm=TRUE) - min(val.parsed, na.rm=TRUE)
			iqr <- IQR(val.parsed, na.rm=TRUE)
			return(c(modeNum,kur,skw,avg,med,sdv,rng,iqr))
		} #else
	} # function
	) # sapply
	) # t
	) # as.data.frame
	colnames(tmp) <- c("modeNum","kur","skw","avg","med","sdv","rng","iqr")
	newdata <- cbind(data, tmp)
}
train <- extract(train)
test <- extract(test)
save(train, file="C:/Kaggle/ELO/train.Rda")
save(test,  file="C:/Kaggle/ELO/test.Rda")

Animations {
row <- 4 # param1
sleeptime <- .1 #param2
val <- as.integer(unlist(strsplit(as.character(test$Valuations[row]), split=" ")))	
val.wo6 <- val[-(1:6)]
valDensity <- density(val.wo6, na.rm=TRUE)$y
modeNum <- length(which(diff(sign(diff(valDensity)))==-2)+1) # http://stackoverflow.com/questions/6836409/finding-local-maxima-and-minima
val.large <- ifelse(abs(val.wo6)>300, (val.wo6/abs(val.wo6))*(300+sqrt(abs(abs(val.wo6)-300))), val.wo6)

# Densities
for(i in 1:45) {
	plot(density(runmean(x=val.large, k=i, alg="C", endrule="trim")))
	Sys.sleep(sleeptime )
}
for(i in 1:45) {
	plot(density(runmean(x=val.parsed, k=i, alg="C", endrule="trim")))
	Sys.sleep(sleeptime )
}

# Valuations
for(i in 1:45) {
	plot(runmean(x=val.large, k=i, alg="C", endrule="trim"), type='l')
	Sys.sleep(sleeptime )
}

for(i in 1:45) {
	plot(runmean(x=val.parsed, k=i, alg="C", endrule="trim"), type='l')
	Sys.sleep(sleeptime)
}
}

# be weary of exchanges, valuation will go from 20 to 300 to 20 and the 300 shouldn't be looked at
# look at every other valuation? find who has control by finding who initiates an exchange.. are exchanges notable?
# try a different smoothing technique, like HoltWinters instead of MA?
# it seems when the result is 0-1 or 1-0, the variance starts small and grows large;
# so its not heteroskedastistic.. could you model the change in variance by taking a 
# moving variance (as oppoed to MA)?
# look at the games where the higher rated player lost and look for statistics that indicate the quality of play was more equal than expected
# diff(val, lag=2) shows you the difference from white turn to white turn and from left turn to left turn
# look at mean(diff(val,lag=2))
# if the large jump in valuation (from -200 to -600) happens on whites turn, then isn't it white's fault (assuming the valuation isn't made up on the next play)